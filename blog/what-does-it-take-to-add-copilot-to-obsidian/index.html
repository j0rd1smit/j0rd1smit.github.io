<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.112.3"><title>What does it take to add Copilot to Obsidian? | Jordi Smit</title><meta name=description content="Ever wanted to have Copilot-like completions in Obsidian? It is now possible. This blog post explains how I created the Copilot Auto Completion plugin for Obsidian."><meta name=keywords content="Obsidian,Copilot,LLM,AI"><link rel=canonical href=https://jordismit.com/blog/what-does-it-take-to-add-copilot-to-obsidian/><meta property="og:type" content="website"><meta property="og:title" content="What does it take to add Copilot to Obsidian? | Jordi Smit"><meta property="og:description" content="Ever wanted to have Copilot-like completions in Obsidian? It is now possible. This blog post explains how I created the Copilot Auto Completion plugin for Obsidian."><meta property="og:site_name" content="Jordi Smit"><meta property="og:url" content="https://jordismit.com/blog/what-does-it-take-to-add-copilot-to-obsidian/"><meta property="og:locale" content="en"><meta property="og:image" content="https://jordismit.com/blog/what-does-it-take-to-add-copilot-to-obsidian/images/cover.jpg"><meta property="og:image:secure_url" content="https://jordismit.com/blog/what-does-it-take-to-add-copilot-to-obsidian/images/cover.jpg"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="What does it take to add Copilot to Obsidian?"><meta name=twitter:description content="Ever wanted to have Copilot-like completions in Obsidian? It is now possible. This blog post explains how I created the Copilot Auto Completion plugin for Obsidian."><link rel=stylesheet href=https://jordismit.com/css/bootstrap.min.7633b7c0c97d19e682feee8afa2738523fcb2a14544a550572caeecd2eefe66b.css integrity="sha256-djO3wMl9GeaC/u6K+ic4Uj/LKhRUSlUFcsruzS7v5ms="><link rel=stylesheet href=https://jordismit.com/css/goolge-fonts.min.59f98c25da594b2a690835ae556add4bd6b87b555c2be822e4f8d7ed2ac28c55.css integrity="sha256-WfmMJdpZSyppCDWuVWrdS9a4e1VcK+gi5PjX7SrCjFU="><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.0.13/css/all.css integrity=sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp crossorigin=anonymous><link rel=stylesheet href=https://jordismit.com/css/medium.min.edbfa243cc780784f7bc1e894ffbbe0f61381ecc59ea59a8e9736b0adbf727c0.css integrity="sha256-7b+iQ8x4B4T3vB6JT/u+D2E4HsxZ6lmo6XNrCtv3J8A="><link rel=stylesheet href=https://jordismit.com/css/additional.min.df77e97a7aaac1ae6e2eb40724704e1fe7702fe8e085c5bc4b21c2b013c90307.css integrity="sha256-33fpenqqwa5uLrQHJHBOH+dwL+jghcW8SyHCsBPJAwc="><link rel=icon type=image/x-icon href=https://jordismit.com/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://jordismit.com/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://jordismit.com/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://jordismit.com/favicon/favicon-16x16.png><meta name=theme-color content="#fff"><script async src="https://www.googletagmanager.com/gtag/js?id=G-YZP2SLQQK7"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YZP2SLQQK7",{anonymize_ip:!1})}</script></head><body class="d-flex flex-column min-vh-100"><nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down"><div class="container pr-0"><a class=navbar-brand href=https://jordismit.com/><span style=font-family:Righteous>Jordi Smit</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarMediumish aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarMediumish><ul class="navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/blog>Blog</a></li><li class=nav-item><a class=nav-link href=/til>TIL</a></li><li class=nav-item><a class=nav-link href=/cheat-sheets>Cheat-sheets</a></li><li class=nav-item><a class=nav-link href=/about>About Me</a></li></ul></div></div></nav><div class=site-content><div class=container><div class=main-content><div class=container><div class=row><div class="col-md-2 pl-0"><div class="share sticky-top sticky-top-offset"><p>Share</p><ul><li class="ml-1 mr-1"><a target=_blank href="https://twitter.com/intent/tweet?text=What%20does%20it%20take%20to%20add%20Copilot%20to%20Obsidian%3f&url=https%3a%2f%2fjordismit.com%2fblog%2fwhat-does-it-take-to-add-copilot-to-obsidian%2f" onclick='return window.open(this.href,"twitter-share","width=550,height=435"),!1'><i class="fab fa-twitter"></i></a></li><li class="ml-1 mr-1"><a target=_blank href="https://facebook.com/sharer.php?u=https%3a%2f%2fjordismit.com%2fblog%2fwhat-does-it-take-to-add-copilot-to-obsidian%2f" onclick='return window.open(this.href,"facebook-share","width=550,height=435"),!1'><i class="fab fa-facebook-f"></i></a></li><li class="ml-1 mr-1"><a target=_blank href="https://www.linkedin.com/sharing/share-offsite/?url=https%3a%2f%2fjordismit.com%2fblog%2fwhat-does-it-take-to-add-copilot-to-obsidian%2f" onclick='return window.open(this.href,"linkedin-share","width=550,height=435"),!1'><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="col-md-9 flex-first flex-md-unordered"><div class=mainheading><div class="row post-top-meta"><div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0 md-nopad-right"><a href=/about><img class=author-thumb src=https://jordismit.com/media/images/author_hu25d333fcded103db2f52f7476cb1420a_204682_250x0_resize_q75_box.jpg alt=Author></a></div><div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-start md-nopad-left"><a href=/about class=link-dark>Jordi
Smit</a><br><span class=author-description>Machine Learning Engineer<br>30 Sep 2023
<i class="far fa-clock clock"></i>
9 min read</span></div></div><h1 class=posttitle>What does it take to add Copilot to Obsidian?</h1></div><div class=post-featured-image><img class=img-fluid src=https://jordismit.com/blog/what-does-it-take-to-add-copilot-to-obsidian/images/cover.jpg alt="thumbnail for this post"></div><div class=article-post><p>Ever since I got access to GitHub Copilot, I have been truly amazed by its capabilities.
It continuously keeps feeding me possible completions for my code and text.
They might not always be perfect, but they are often good enough to be used as a starting point and prevent me from suffering from the white page syndrome.
I&rsquo;m also an avid user of Obsidian, a note-taking application, where I often encounter the same white page syndrome.
This often resulted in me either writing my longer notes in my IDE using Copilot or procrastinating and not writing the note (or blog) at all.
The engineer in me saw this as a challenge, which resulted in the <a href="https://obsidian.md/plugins?search=Copilot%20auto%20completion">Obsidian Copilot plugin</a>.
Like the name suggests, this plugin adds Copilot-like auto-completion to Obsidian with the help of the OpenAI API, which looks something like this:</p><figure><img src=images/demo.gif alt="An example where the plugin suggests a Latex formula based on all the text in the note."><figcaption><p>An example where the plugin suggests a Latex formula based on all the text in the note.</p></figcaption></figure><p>You might be wondering what does it take to implement a Copilot-like plugin for Obsidian?
It turns out that all comes down to three questions:</p><ul><li>How do we obtain completions?</li><li>How do we ensure that the obtained completions are the type of completions we want?</li><li>How do we design a maintainable software architecture for such a plugin?</li></ul><h2 id=obtaining-completions>Obtaining completions</h2><p>The thing that makes Copilot so powerful is that its predictions take the text before and after your cursor into account.
If you know a bit about transformers, you might think that is a bit strange since autoregressive transformers like GPT-3 only take previous tokens into account in their predictions.
So, how do you force an autoregressive transformer-based model to take the text before and after the cursor into account in its prediction?
This problem is solvable with some clever, prompt engineering.
We tell the model that we give it some text with the format <code>&lt;text_before_cursor> &lt;mask/> &lt;text_after_cursor>,</code> and its task is to respond with the most logical words that can replace the <code>&lt;mask/></code>.
In the plugin, we do this using the following system prompt:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>Your job is to predict the most logical text that should be written at the location of the <span style=color:#e6db74>`&lt;mask/&gt;`</span>.
</span></span><span style=display:flex><span>Your answer can be either code, a single word, or multiple sentences.
</span></span><span style=display:flex><span>Your answer must be in the same language as the text that is already there.
</span></span><span style=display:flex><span>Your response must have the following format:
</span></span><span style=display:flex><span>THOUGHT: here you explain your reasoning of what could be at the location of <span style=color:#e6db74>`&lt;mask/&gt;`</span>
</span></span><span style=display:flex><span>ANSWER: here you write the text that should be at the location of <span style=color:#e6db74>`&lt;mask/&gt;`</span>
</span></span></code></pre></div><p>We then provide the model with the text before and after the cursor, which might look something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># The Softmax &lt;mask/&gt;
</span></span><span style=display:flex><span>The softmax function transforms a vector into a probability distribution such that the sum of the vector is equal to 1
</span></span></code></pre></div><p>The model will then response with something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>THOUGHT: <span style=color:#e6db74>`&lt;mask/&gt;`</span> is located inside a Markdown headings. The header already contains the text &#34;The Softmax&#34; contains so my answer should be coherent with that. The text after <span style=color:#e6db74>`&lt;mask/&gt;`</span> is about the softmax function, so the title should reflect this.
</span></span><span style=display:flex><span>ANSWER: function
</span></span></code></pre></div><p>The text after <code>THOUGHT:</code> allows the model to reason a bit about the context around the cursor before writing the answer in the <code>ANSWER:</code> section.
This is a prompt engineering trick called <a href=https://www.promptingguide.ai/techniques/cot>Chain-of-Thought</a>.
The idea here is that if a model explains its reasoning, it will be more likely to write a coherent answer since it gives the attention mechanism more guidance.
For our use case, this works remarkably well.
We are mainly interested in the <code>ANSWER:</code> section since contains the actual completion.
Thanks to the <code>ANSWER:</code> prefix, we can extract this text using regex, which is exactly what we do in the plugin.</p><h3 id=making-the-model-context-aware>Making the model context-aware</h3><p>We now have a way to obtain completions from the model thanks to the system prompt above.
However, when I used the above system prompt, I noticed that the model often generated generic text completions independent of the cursor location.
For example, even in Python code blocks, the model preferred to generate English text completions instead of Python code.
This is not what we want.
We humans expect different types of completions in different cursor locations, for example:</p><ul><li>If the cursor is inside a Python code block, we expect a completion with Python code.</li><li>If the cursor is inside a math block, we expect a completion with latex formulas.</li><li>If the cursor is inside a list, we expect a new list item.</li><li>If the cursor is inside a heading, we expect a new heading that represents the paragraph&rsquo;s content.</li><li>Etc.</li></ul><p>You can probably think of many more examples and expectations.
Encoding all these expectations in the system prompt would make it very long and complex.
Instead, it is easier to use a <a href=https://www.promptingguide.ai/techniques/fewshot>few-shot example</a> approach.
In this approach, we give the model some example input and output pairs that implicitly show the model what we expect in the response for the given context.
For example, when the cursor is inside a Math block, we give the following example input:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># Logarithm definition
</span></span><span style=display:flex><span>A logarithm is the power to which a base must be raised to yield a given number.
</span></span><span style=display:flex><span>For example $2^3 =8$; therefore, 3 is the logarithm of 8 to base 2, or in other words $&lt;<span style=color:#f92672>mask</span>/&gt;$.
</span></span></code></pre></div><p>Combined with the following example output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>THOUGHT: The &lt;<span style=color:#f92672>mask</span>/&gt; is located inline math block. 
</span></span><span style=display:flex><span>    The text before the mask is about logarithm. 
</span></span><span style=display:flex><span>    The text is giving an example but the math notation still needs to be completed. 
</span></span><span style=display:flex><span>    So my answer should be the latex formula for this example.  
</span></span><span style=display:flex><span>ANSWER: 3 = \log_2(8)
</span></span></code></pre></div><p>Examples like this allow us to implicitly show the model what kind of responses we expect in specific cursor locations.
However, they also have another big advantage.
The prompt and examples can be dynamic and context-specific.
For example, if the cursor is inside a Code block, we only include the few-shot examples related to code blocks.
If the cursor is inside a math block, we only select the few-shot examples related to math and latex formulas, etc.
This way, we can make the model context-aware without encoding all the context into one long system prompt.
Allowing us to reduce the prompt length, complexity, and inference costs.</p><figure><img src=images/few-shot-example-visual.gif alt="Some examples of how the prompt changes dynamically based on the cursor&amp;rsquo;s location."><figcaption><p>Some examples of how the prompt changes dynamically based on the cursor&rsquo;s location.</p></figcaption></figure><p>Another nice side effect of this few-shot example approach is that it allows users to customize the type of completions they expect.
Maybe a user wants the model to write in their native language?
Or the user might want the model to generate to-do list tasks in their specific style.
All they need to do is write an example input and model response for a given context, and the model will learn to generate tasks in your style.
This makes the model very flexible and customizable to the user&rsquo;s needs.
That is why the plugin allows users to edit the existing few-shot examples or add their own via the settings menu.</p><figure><img src=images/few-shot-examples-setting.jpg alt="The settings menu where users can add or change few-shot examples per cursor context."><figcaption><p>The settings menu where users can add or change few-shot examples per cursor context.</p></figcaption></figure><h2 id=plugin-architecture>Plugin architecture</h2><p>We now have a way to obtain completions from the model and a method to ensure that the model generates the type of completions we expect.
Now, a big question remains: how do we integrate this into an IDE-like Obsidian without making the code needlessly complex?
The code for a plugin like this can quickly become complex because it must listen to many different events and perform different actions for the same event in different situations.
For example, when the user presses the <code>Tab</code> key, a lot of things can happen:</p><ul><li>If the plugin shows a completion, the plugin should insert the completion.</li><li>If the user is just typing, the plugin should do nothing to let the default Obsidian behavior take over.</li><li>If a prediction is queued, the plugin should cancel the prediction since the prediction context is outdated while still letting the default Obsidian behavior take over.</li><li>Etc.</li></ul><p>As you can see, with just this single event as an example, the plugin&rsquo;s behavior can quickly become complex while we still need to handle many more events.
If you are not careful, you will end up with a highly complex plugin filled with if-else statements that are impossible to maintain and extend.
Luckily, this problem has a solution: the <a href=https://refactoring.guru/design-patterns/state>state machine</a> design pattern.</p><p>When you think about it, the plugin has five different situations (or states) it can be in:</p><ul><li><strong>Idle</strong>: The plugin is enabled, awaiting a user event that triggers a prediction.</li><li><strong>Queued</strong>: A trigger has been detected, and the plugin waits for the trigger delay to expire before making a prediction. This delay is needed to minimize the number of API calls (and inference costs).</li><li><strong>Predicting</strong>: A prediction request to the API provider and is waiting for the response.</li><li><strong>Suggesting</strong>: A completion has been generated and shown to the user, who can accept or reject it.</li><li><strong>Disabled</strong>: The plugin is disabled, and all events are ignored.</li></ul><p>Depending on the event that occurred, the plugin will transition from one state to another, as shown in the figure below.</p><figure><img src=images/state_diagram.jpg alt="All the possible plugin states and transitions."><figcaption><p>All the possible plugin states and transitions.</p></figcaption></figure><p>The big advantage of this approach is that we group all the state-specific behavior code in one place.
For example, all the idle state-specific behavior code is grouped in the <code>IdleState</code> class.
This code is much easier to understand and reason about than many, possibly nested, if-else statements.
Another big advantage is that you can visualize the plugin&rsquo;s behavior in a state diagram like the one above, making it easier to explain the code&rsquo;s behavior to new developers.
These things make the plugin&rsquo;s codebase much easier to maintain and extend.</p><h2 id=wrapping-up>Wrapping up</h2><p>You now have a basic understanding of how the Obsidian Copilot plugin works and what it took to build it.
Now, you might wonder, &ldquo;How well does it work?&rdquo; and &ldquo;Does it really help avoid the white page syndrome?&rdquo;
Of course, I used this plugin to help me write this blog post, and since you are reading this, I think it is safe to say that it helped me avoid the white page syndrome.
So, it might be worth giving it a try yourself. You can find the plugin in the <a href="https://obsidian.md/plugins?search=Copilot%20auto%20completion">Obsidian community plugin store</a> and the code on <a href=https://github.com/j0rd1smit/obsidian-copilot-auto-completion>GitHub</a>.
Enjoy!</p></div><div class=after-post-tags><ul class=tags><li><a href=/tags/obsidian>Obsidian</a></li><li><a href=/tags/copilot>Copilot</a></li><li><a href=/tags/llm>LLM</a></li><li><a href=/tags/ai>AI</a></li></ul></div><div class="row PageNavigation d-flex justify-content-between font-weight-bold"><a class="d-block col-md-6" href=https://jordismit.com/blog/practicing-your-dbt-skills-locally-with-duckdb/>&#171; Practicing your DBT skills locally with DuckDB</a>
<a class="d-block col-md-6 text-lg-right" href=https://jordismit.com/blog/adding-application-insight-based-monitoring-to-fast-api/>Adding Application Insight Based Monitoring to Fast API &#187;</a><div class=clearfix></div></div></div></div></div></div></div></div><footer class="footer mt-auto"><div class=container><div class=row><div class="col-md-12 col-sm-612 text-center">&copy; Copyright
Jordi Smit - All rights reserved</div></div></div></footer><script async src=https://jordismit.com/js/mediumish.min.f5bb3d8c672aaed326977e05f0187a6b8b165e033478c286160ebac554991ee1.js integrity="sha256-9bs9jGcqrtMml34F8Bh6a4sWXgM0eMKGFg66xVSZHuE="></script></body></html>