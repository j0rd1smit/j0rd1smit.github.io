<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.111.3"><title>DIY auto-grad Engine: A Step-by-Step Guide to Calculating Derivatives Automatically | Jordi Smit</title><meta name=description content="Want to understand the magic of Jax, PyTorch, and TensorFlow auto-grad engines? The best to learn is to build your DIY version from scratch in Python."><meta name=keywords content="python,deep learning,autograd"><link rel=canonical href=https://jordismit.com/blog/diy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically/><meta property="og:type" content="website"><meta property="og:title" content="DIY auto-grad Engine: A Step-by-Step Guide to Calculating Derivatives Automatically | Jordi Smit"><meta property="og:description" content="Want to understand the magic of Jax, PyTorch, and TensorFlow auto-grad engines? The best to learn is to build your DIY version from scratch in Python."><meta property="og:site_name" content="Jordi Smit"><meta property="og:url" content="https://jordismit.com/blog/diy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically/"><meta property="og:locale" content="en"><meta property="og:image" content="https://jordismit.com/blog/diy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically/images/cover.png"><meta property="og:image:secure_url" content="https://jordismit.com/blog/diy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically/images/cover.png"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="DIY auto-grad Engine: A Step-by-Step Guide to Calculating Derivatives Automatically"><meta name=twitter:description content="Want to understand the magic of Jax, PyTorch, and TensorFlow auto-grad engines? The best to learn is to build your DIY version from scratch in Python."><link rel=stylesheet href=https://jordismit.com/css/bootstrap.min.60b19e5da6a9234ff9220668a5ec1125c157a268513256188ee80f2d2c8d8d36.css integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY="><link rel=stylesheet href=https://jordismit.com/css/goolge-fonts.b251d02039bb76d3217f1bec88330f4bc1721d311854924a7217dcffe5bc80cb.css integrity="sha256-slHQIDm7dtMhfxvsiDMPS8FyHTEYVJJKchfc/+W8gMs="><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.0.13/css/all.css integrity=sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp crossorigin=anonymous><link rel=stylesheet href=https://jordismit.com/css/medium.34da69acb5cb8e4275b44045c1ffd31648ac0f9f119c63ca72a121216a1be4c5.css integrity="sha256-NNpprLXLjkJ1tEBFwf/TFkisD58RnGPKcqEhIWob5MU="><link rel=stylesheet href=https://jordismit.com/css/additional.4ef3d39c77b4a1adf1b5b6998bc22aa9fe2465cbbfbfdc12c2b7aa07dc13dd9d.css integrity="sha256-TvPTnHe0oa3xtbaZi8Iqqf4kZcu/v9wSwreqB9wT3Z0="><link rel=icon type=image/x-icon href=https://jordismit.com/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://jordismit.com/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://jordismit.com/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://jordismit.com/favicon/favicon-16x16.png><meta name=theme-color content="#fff"></head><body class="d-flex flex-column min-vh-100"><nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down"><div class="container pr-0"><a class=navbar-brand href=https://jordismit.com/><span style=font-family:Righteous>Jordi Smit</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarMediumish aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarMediumish><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/blog>Blog</a></li><li class=nav-item><a class=nav-link href=/til>TIL</a></li><li class=nav-item><a class=nav-link href=/cheat-sheets>Cheat-sheets</a></li><li class=nav-item><a class=nav-link href=/about>About Me</a></li></ul></div></div></nav><div class=site-content><div class=container><div class=main-content><div class=container><div class=row><div class="col-md-2 pl-0"><div class="share sticky-top sticky-top-offset"><p>Share</p><ul><li class="ml-1 mr-1"><a target=_blank href="https://twitter.com/intent/tweet?text=DIY%20auto-grad%20Engine%3a%20A%20Step-by-Step%20Guide%20to%20Calculating%20Derivatives%20Automatically&url=https%3a%2f%2fjordismit.com%2fblog%2fdiy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically%2f" onclick='return window.open(this.href,"twitter-share","width=550,height=435"),!1'><i class="fab fa-twitter"></i></a></li><li class="ml-1 mr-1"><a target=_blank href="https://facebook.com/sharer.php?u=https%3a%2f%2fjordismit.com%2fblog%2fdiy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically%2f" onclick='return window.open(this.href,"facebook-share","width=550,height=435"),!1'><i class="fab fa-facebook-f"></i></a></li><li class="ml-1 mr-1"><a target=_blank href="https://www.linkedin.com/sharing/share-offsite/?url=https%3a%2f%2fjordismit.com%2fblog%2fdiy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically%2f" onclick='return window.open(this.href,"linkedin-share","width=550,height=435"),!1'><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="col-md-9 flex-first flex-md-unordered"><div class=mainheading><div class="row post-top-meta"><div class="col-xs-12 col-md-3 col-lg-2 text-center text-md-left mb-4 mb-md-0 md-nopad-right"><a href=/about><img class=author-thumb src=https://jordismit.com/media/images/author_hu25d333fcded103db2f52f7476cb1420a_204682_250x0_resize_q75_box.jpg alt=Author></a></div><div class="col-xs-12 col-md-9 col-lg-10 text-center text-md-left md-nopad-left"><a href=/about class=link-dark>Jordi
Smit</a><br><span class=author-description>Machine Learning Engineer<br>Dec 22, 2022
<i class="far fa-clock clock"></i>
10 min read</span></div></div><h1 class=posttitle>DIY auto-grad Engine: A Step-by-Step Guide to Calculating Derivatives Automatically</h1></div><div class=post-featured-image><img class=img-fluid src=https://jordismit.com/blog/diy-auto-grad-engine-a-step-by-step-guide-to-calculating-derivatives-automatically/images/cover.png alt="thumbnail for this post"></div><div class=article-post><p>Have you ever marveled at the magic of PyTorch, Jax, and Tensorflow&rsquo;s auto-grad engines?
I know I have!
I recently decided to take on the challenge of implementing an auto-grad engine from scratch in Python.
It was a fun and humbling experience, and now I want to share my knowledge with you.
In this step-by-step guide, I&rsquo;ll walk you through the process of building your very own DIY auto-grad engine.
At the end of this blog, you will understand how these engines automatically calculate derivatives and how you can implement this in Python from scratch.
So if you&rsquo;re ready to dive into the exciting world of auto-grad engines, let&rsquo;s get started!</p><h2 id=differentiation-refresher>Differentiation refresher</h2><p>Before we start, let’s ensure we’re all on the same math page.
If you still know all the differentiation rules from high school, feel free to skip this section.
If you need a quick refresher, read on!</p><p>Differentiation is a mathematical operation that allows us to find the rate of change of a function with respect to one of its variables.
So, if we write $\frac{\partial L}{\partial x}$, we mean the rate of change of $L$ with respect to $x$.
If you have a function $L(x)$, you can find the derivative of $L$ with respect to $x$ by applying the differentiation rules, which rule you apply depends on the function $L$.
Luckily, you don&rsquo;t need to remember all the rules because I have created a nice look-up table for you with all the rules you need to know for this post.</p><table class="table table-striped table-bordered"><thead><tr><th>Name</th><th style=text-align:center>Function</th><th style=text-align:center>Derivative</th></tr></thead><tbody><tr><td>Definition</td><td style=text-align:center>-</td><td style=text-align:center>$\frac{\partial L}{\partial L} = 1$</td></tr><tr><td>Sum rule</td><td style=text-align:center>$L = x_1 + x_2$</td><td style=text-align:center>$\frac{\partial L}{\partial x_1} = 1$, $\frac{\partial L}{\partial x_2} = 1$</td></tr><tr><td>Multiplication rule</td><td style=text-align:center>$L = x_1 * x_2$</td><td style=text-align:center>$\frac{\partial L}{\partial x_1} = x_2$, $\frac{\partial L}{\partial x_2} = x_1$</td></tr><tr><td>Chain rule</td><td style=text-align:center>-</td><td style=text-align:center>$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial x} \frac{\partial y}{\partial y} = \frac{\partial L}{\partial y} \frac{\partial y}{\partial x}$</td></tr></tbody></table><p>After you have finished this post, you won’t even need to use this look-up table anymore because your computer will be able to apply all these rules for you automatically!
However, before we get there, let’s first review the backpropagation algorithm.</p><h2 id=back-propagation>Back propagation</h2><p>Backpropagation is an algorithm that automatically calculates the functions&rsquo; derivatives.
It works by constructing a computational graph and then iteratively calculating the gradients of each node based on the chain rule.
I can imagine that you&rsquo;re already thinking: &ldquo;Well, that sounds a bit vague to me&rdquo;.
So let&rsquo;s see how this work using a running example.
We want to calculate the derivatives of $L$ with respect to every $x_i$:</p><p>$$
x_1 = 3
$$
$$
x_2 = 4
$$
$$
x_3 = x_1 * x_2 = 12
$$
$$
x_4 = 5
$$
$$
x_5 = x_3 + x_4
$$
$$
x_6 = 2
$$
$$
L = 34
$$</p><p>The first thing we need to do is transform this function into a computational graph.
For example, we can view the equation above as the following graph:</p><figure><img src=images/computational_graph.png alt="A visualization of the computational graph."><figcaption><p>A visualization of the computational graph.</p></figcaption></figure><p>The goal is to find the gradients with respect to $L$ for every node in this computational graph.
Calculating the gradients for $x_1$ and $x_2$ looks quite tricky.
However, it is a bit easier to find the calculating gradients for $x_5$ and $x_6$ since we can look up their derivation rules in the table above.
So, according to the table above, we have:</p><p>$$
\frac{\partial L}{\partial L} = 1
$$
$$
\frac{\partial L}{\partial x_5} = x_6 = 2
$$
$$
\frac{\partial L}{\partial x_6} = x_5 = 17
$$
So, let&rsquo;s update our computational graph with these gradients:</p><figure><img src=images/computation_graph_with_gradients_partial_1.jpg alt="A visualization of the computational graph and the gradients so far."><figcaption><p>A visualization of the computational graph and the gradients so far.</p></figcaption></figure><p>Now, the question is, how do we calculate the gradients for $x_3$ and $x_4$?
We know the gradients of $x_5$, which is the parent node of $x_3$ and $x_4$.
So, we can use the chain rule and the sum rule to rewrite the gradients for $x_3$ and $x_4$ as follows:</p><p>$$
\frac{\partial L}{\partial x_3} = \frac{\partial L}{\partial x_3} \frac{\partial x_5}{\partial x_5} = \frac{\partial L}{\partial x_5} \frac{\partial x_5}{\partial x_3} = 2 *1 = 2
$$</p><p>$$
\frac{\partial L}{\partial x_4} = \frac{\partial L}{\partial x_4} \frac{\partial x_5}{\partial x_5} = \frac{\partial L}{\partial x_5} \frac{\partial x_5}{\partial x_4} = 2* 1 = 2
$$</p><p>So, by using the chain rule, we change the gradient formula into the local gradient times the gradient of the parent node, which we both know.
Let’s continue by filling in these results into our computational graph:</p><figure><img src=images/computation_graph_with_gradients_partial_2.jpg alt="A visualization of the computational graph and the gradients so far."><figcaption><p>A visualization of the computational graph and the gradients so far.</p></figcaption></figure><p>Thanks to the previous step, the gradient of the parent node of x_1 and x_2 is now known.
Therefore, we can again use the chain rule to change the initially complex derivative formulas into the local gradients times the gradient of the parent.
$$
\frac{\partial L}{\partial x_1} = \frac{\partial L}{\partial x_1} \frac{\partial x_3}{\partial x_3} = \frac{\partial L}{\partial x_3} \frac{\partial x_5}{\partial x_1} = 2 *3 = 6
$$</p><p>$$
\frac{\partial L}{\partial x_2} = \frac{\partial L}{\partial x_2} \frac{\partial x_3}{\partial x_3} = \frac{\partial L}{\partial x_3} \frac{\partial x_5}{\partial x_2} = 2* 4 = 8
$$
This give finally the following computational graph:<figure><img src=images/computation_graph_with_gradients.png alt="A visualization of the computational graph with all gradients."><figcaption><p>A visualization of the computational graph with all gradients.</p></figcaption></figure></p><p>We have now calculated the gradients for every node in the computational graph.
With these gradients, we know exactly how changes to any of the $x_i$ will impact the value of $L$.
So, let summarize what we have done so far:</p><ol><li>We have constructed a computational graph of the function $L$.</li><li>We set the gradients of the leaf node $L$ to 1.</li><li>We iteratively calculated the gradients of each node in reversed topological order based on the chain rule and the local differentiation rules from the table above.</li></ol><p>It is nice that we know how to do this manually, but it is a bit tedious.
So, let&rsquo;s see how we can automate this using an auto-grad engine.</p><h2 id=implementing-the-autograd-engine>Implementing the Autograd Engine</h2><p>In this section, we will focus on building a Python data structure that keeps track of the computational graph and local gradients.
This will allow us to perform automatic differentiation, which is the key to implementing an autograd engine.
To keep track of the computational graph and local gradients, we will implement a <code>Value</code> class. This class will have the following attributes:</p><ul><li><code>data</code>: A float representing the current value of the node in the computational graph.</li><li><code>children</code>: A set containing the child nodes that contributed to this computation node.</li><li><code>_backwards</code>: A function that knows how to apply the chain rule to calculate the gradient rule if the parent gradient is known.</li></ul><p>This translates to the following <code>__init__</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Backwards <span style=color:#f92672>=</span> Callable[[float], <span style=color:#66d9ef>None</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_noop_grad</span>(_: float) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Value</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        data: float,
</span></span><span style=display:flex><span>        children: Optional[set[Value]] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        _backwards: Backwards <span style=color:#f92672>=</span> _noop_grad,
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> data
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>children <span style=color:#f92672>=</span> set() <span style=color:#66d9ef>if</span> children <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span> <span style=color:#66d9ef>else</span> children
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_backwards <span style=color:#f92672>=</span> _backwards    
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span></code></pre></div><p>Our auto-grad engine needs to be able to construct a computational graph dynamically.
So, for example, if we add two <code>Value</code> objects together, we want to create a new Value object that represents the sum of the two <code>Value</code> objects while keeping track of the underlying computational graph.
We can keep track of this connection by passing the previous <code>Value</code> objects used to construct this node via the <code>children</code> attribute.
We can do this by implementing the <code>__add__</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Value</span>:
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>def</span> __add__(self, other: Value) <span style=color:#f92672>-&gt;</span> Value:
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_backward</span>(grad_parent: float) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>           self<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> grad_parent
</span></span><span style=display:flex><span>           other<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> grad_parent
</span></span><span style=display:flex><span>   
</span></span><span style=display:flex><span>       <span style=color:#66d9ef>return</span> Value(
</span></span><span style=display:flex><span>           self<span style=color:#f92672>.</span>data <span style=color:#f92672>+</span> other<span style=color:#f92672>.</span>data,
</span></span><span style=display:flex><span>           children<span style=color:#f92672>=</span>{self, other},
</span></span><span style=display:flex><span>           _backwards<span style=color:#f92672>=</span>_backward,
</span></span><span style=display:flex><span>       )
</span></span><span style=display:flex><span>   <span style=color:#f92672>...</span>
</span></span></code></pre></div><p>This implementation of the <code>__add__</code> method does the following:</p><ul><li>It calculates the data value for the new <code>Value</code> object by adding the data values of the two <code>Value</code> objects.</li><li>Here we define how to calculate the gradient of <code>self</code> and <code>other</code> in the <code>_backward</code> function.
This function takes as input the gradient of the parent node and calculates the gradient of the child nodes.
This is the chain rule in action, exactly as we did manually in the previous section.</li><li>It keeps track of the <code>children</code> of the new <code>Value</code> object. This is the computational graph connection.</li></ul><p>Now, we can do a single gradient calculation as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_1 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>x_2 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> x_1 <span style=color:#f92672>+</span> x_2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>l<span style=color:#f92672>.</span>_backwards(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> x_1<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> x_2<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>At this point our <code>_backwards</code> function only works for a computational graph that is a single layer deep.
So, let&rsquo;s extend our auto-grad engine to support more complex computational graphs.
To do this, we need to implement the <code>backward</code> method.
This will set the gradient of the root node to <code>1</code> and then recursively call the <code>_backwards</code> method on all the child nodes in reversed topological order.
You might ask yourself why we need to do this in reversed topological order?
The reason for this is that we can only use the <code>_backwards</code> method if we know the gradient of the parent node.
If we iterate over the node in reversed topological order, the gradient of the parent node is always known when we call the <code>_backwards</code> method.
So, let&rsquo;s implement the <code>backward</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Value</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>backward</span>(self) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> v <span style=color:#f92672>in</span> find_reversed_topological_order(self):
</span></span><span style=display:flex><span>            v<span style=color:#f92672>.</span>_backwards(v<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>find_reversed_topological_order</span>(root: Value) <span style=color:#f92672>-&gt;</span> list[Value]:
</span></span><span style=display:flex><span>    visited <span style=color:#f92672>=</span> set()
</span></span><span style=display:flex><span>    topological_order <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_depth_first_search</span>(node: Value):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> node <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> visited:
</span></span><span style=display:flex><span>            visited<span style=color:#f92672>.</span>add(node)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> child <span style=color:#f92672>in</span> node<span style=color:#f92672>.</span>children:
</span></span><span style=display:flex><span>                _depth_first_search(child)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            topological_order<span style=color:#f92672>.</span>append(node)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    _depth_first_search(root)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> list(reversed(topological_order))
</span></span></code></pre></div><p>In this implementation, we use a depth first search to find the reversed topological order.
Now, with the <code>backwards</code> method implemented, we can do a gradient calculation on deeper and more complex computational graphs as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_1 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>x_2 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>x_3 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>x_4 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> x_1 <span style=color:#f92672>+</span> x_2 <span style=color:#f92672>+</span> x_3 <span style=color:#f92672>+</span> x_4
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>l<span style=color:#f92672>.</span>backwards()
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> l<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> x_1<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> x_2<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> x_3<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> x_4<span style=color:#f92672>.</span>grad <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>That is great!
Our auto-grad engine now works for arbitrary deep computational graphs.
Sadly, it only works with additions.
So, let&rsquo;s extend our auto-grad engine to support more operations.
Let&rsquo;s start with multiplication:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Value</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __mul__(self, other: Value) <span style=color:#f92672>-&gt;</span> Value:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_backward</span>(grad_parent: float) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> other<span style=color:#f92672>.</span>data <span style=color:#f92672>*</span> grad_parent
</span></span><span style=display:flex><span>            other<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>data <span style=color:#f92672>*</span> grad_parent
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Value(
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>data <span style=color:#f92672>*</span> other<span style=color:#f92672>.</span>data,
</span></span><span style=display:flex><span>            children<span style=color:#f92672>=</span>{self, other},
</span></span><span style=display:flex><span>            _backwards<span style=color:#f92672>=</span>_backward,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span></code></pre></div><p>Notice that this looks very similar to the <code>__add__</code> method.
The only differance is that we use the local differentiation rule for multiplication in the <code>_backward</code> function.
Everything else is the same.</p><p>Now, with the multiplication we can verify that our auto-grad engine can solve the computational graph we solved manually in the previous section:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_1 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>x_2 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>x_3 <span style=color:#f92672>=</span> x_1 <span style=color:#f92672>*</span> x_2
</span></span><span style=display:flex><span>x_4 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>x_5 <span style=color:#f92672>=</span> x_4 <span style=color:#f92672>+</span> x_3
</span></span><span style=display:flex><span>x_6 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>l <span style=color:#f92672>=</span> x_5 <span style=color:#f92672>*</span> x_6
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>l<span style=color:#f92672>.</span>backwards()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>x_1<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>x_1<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># date=3 grad=8</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>x_2<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>x_2<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># date=4 grad=6</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>x_3<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>x_3<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># date=12 grad=2</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>x_4<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>x_4<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># date=5 grad=2</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>x_5<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>x_5<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># date=17 grad=2</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>x_6<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>x_6<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>) <span style=color:#75715e># date=2 grad=17</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;date=</span><span style=color:#e6db74>{</span>l<span style=color:#f92672>.</span>date<span style=color:#e6db74>}</span><span style=color:#e6db74> grad=</span><span style=color:#e6db74>{</span>l<span style=color:#f92672>.</span>grad<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)     <span style=color:#75715e># date=34 grad=1</span>
</span></span></code></pre></div><p>Our auto-grad engine now supports addition and multiplication.
Of course there are many more operations that we can support, like subtraction, division, exponentiation, etc.
However, I want to leave those as an exercise for the reader.
To make this exercise a bit easier, I have created a <a href=https://github.com/j0rd1smit/mini_auto_grad>GitHub repository</a> with the code we have written so far.
It also contains a <a href=https://github.com/j0rd1smit/mini_auto_grad#exercise-1-implement-the-computational-graph>test suite</a> that you can use to verify your implementation and solutions if you get stuck.</p><h2 id=conclusion>Conclusion</h2><p>In this blog post, we delved into the backpropagation algorithm and how it can be used to calculate gradients in a computational graph.
We then automated this process by implementing a mini auto-grad engine.
During this process, we learned that auto-grad engines are not magic but rather smart data structures that apply the chain rule automatically.
Despite this, they are handy for calculating gradients with ease.</p><p>I hope this tutorial was helpful and that you were able to follow along and implement your own auto-grad engine.</p><h2 id=references>References</h2><ol><li><a href=https://github.com/j0rd1smit/mini_auto_grad>GitHub repository of my implementation</a></li><li><a href=https://en.wikipedia.org/wiki/Backpropagation>Backpropagation</a></li><li><a href=https://github.com/karpathy/micrograd>Micrograd from andrej karpathy</a></li></ol></div><div class=after-post-tags><ul class=tags><li><a href=/tags/python>python</a></li><li><a href=/tags/deep-learning>deep learning</a></li><li><a href=/tags/autograd>autograd</a></li></ul></div><div class="row PageNavigation d-flex justify-content-between font-weight-bold"><a class="d-block col-md-6" href=https://jordismit.com/blog/adding-application-insight-based-monitoring-to-fast-api/>&#171; Adding Application Insight Based Monitoring to Fast API</a>
<a class="d-block col-md-6 text-lg-right" href=https://jordismit.com/blog/python-dataclass-from-scratch/>Python Dataclass From Scratch &#187;</a><div class=clearfix></div></div></div></div></div></div></div></div><footer class="footer mt-auto"><div class=container><div class=row><div class="col-md-12 col-sm-612 text-center">&copy; Copyright
Jordi Smit - All rights reserved</div></div></div></footer><script src=https://jordismit.com/js/jquery-3.6.1.min.131c0d82967fed05e1920e519e0ea6ec91ab97b7c40480f72f8af8680bba1f0a.js integrity="sha256-ExwNgpZ/7QXhkg5Rng6m7JGrl7fEBID3L4r4aAu6Hwo="></script>
<script src=https://jordismit.com/js/popper.min.549cf842cba3739e48efdb4fb6c06405d0e14a02e274538dcd9eed23f49b9e98.js integrity="sha256-VJz4Qsujc55I79tPtsBkBdDhSgLidFONzZ7tI/Sbnpg="></script>
<script src=https://jordismit.com/js/bootstrap.min.0a34a87842c539c1f4feec56bba982fd596b73500046a6e6fe38a22260c6577b.js integrity="sha256-CjSoeELFOcH0/uxWu6mC/Vlrc1AARqbm/jiiImDGV3s="></script>
<script src=https://jordismit.com/js/mediumish.d6fc184863fad23419eee1ce39d87152fb7f8a1c2444f5da3d16b5efa375e389.js integrity="sha256-1vwYSGP60jQZ7uHOOdhxUvt/ihwkRPXaPRa176N144k="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></body></html>